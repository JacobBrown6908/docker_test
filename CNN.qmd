
```{python}
# Import libraries
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.models import Sequential
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import matplotlib.pyplot as plt
import numpy as np
import random
import os
```


```{python}
path=""
target_names = os.listdir(path)
target_names
```

```{python}
data_augmentation = tf.keras.Sequential([
    tf.keras.layers.RandomFlip("horizontal_and_vertical"),  
    tf.keras.layers.RandomRotation(0.8),                   
    tf.keras.layers.RandomZoom(0.8),                       
    tf.keras.layers.RandomContrast(0.7),
    tf.keras.layers.RandomBrightness(0.8),
    tf.keras.layers.CenterCrop(height=224, width=224),
    tf.keras.layers.RandomCrop(height=180, width=180),
    tf.keras.layers.RandomCrop(height=180, width=180),  # Simulate RandomResizedCrop
    tf.keras.layers.Resizing(height=224, width=224),    # Resizing step
])

# Data Loading Function
def load_train_val_data(path, img_size, batch_size, validation_split=0.2, seed=123):
    # Load Training Data
    train_dataset = tf.keras.preprocessing.image_dataset_from_directory(
        path,
        labels='inferred',
        label_mode='int',
        shuffle=True,
        image_size=img_size,
        batch_size=batch_size,
        color_mode='rgb',
        validation_split=validation_split,
        subset='training',
        seed=seed,
    )

    # Apply data augmentation to training dataset
    train_dataset = train_dataset.map(lambda x, y: (data_augmentation(x, training=True), y))

    # Load Validation Data (no augmentation)
    val_dataset = tf.keras.preprocessing.image_dataset_from_directory(
        path,
        labels='inferred',
        label_mode='int',
        shuffle=False,  # Avoid shuffling for consistent validation
        image_size=img_size,
        batch_size=batch_size,
        color_mode='rgb',
        validation_split=validation_split,
        subset='validation',
        seed=seed,
    )

    return train_dataset, val_dataset
```


```{python}
train_dataset, val_dataset = load_train_val_data(
    path= path,
    img_size=(224, 224),
    batch_size=50,
    validation_split=0.3,
    seed=947
)
```

```{python}
# Build a model...
img_size = (224, 224)

num_classes = len(target_names)

model = Sequential([
  tf.keras.layers.Conv2D(32, (4, 4), activation='relu', input_shape=(img_size[0], img_size[1], 3)),
  tf.keras.layers.MaxPool2D((2, 2), padding='valid'),
  tf.keras.layers.Conv2D(64, (4, 4), activation='relu'),
  tf.keras.layers.MaxPool2D((2, 2), padding='valid'),
  tf.keras.layers.Conv2D(128, (4, 4), activation='relu'),
  tf.keras.layers.MaxPool2D((2, 2), padding='valid'),
  tf.keras.layers.Conv2D(256, (4, 4), activation='relu'),
  tf.keras.layers.MaxPool2D((2, 2), padding='valid'),
  tf.keras.layers.Conv2D(512, (4, 4), activation='relu'),
  tf.keras.layers.MaxPool2D((2, 2), padding='valid'),
  tf.keras.layers.Flatten(),
  tf.keras.layers.Dropout(0.5),
  tf.keras.layers.Dense(512, activation='relu'),
  tf.keras.layers.Dense(512, activation='relu'),
  tf.keras.layers.Dropout(0.4),
  tf.keras.layers.Dense(256, activation='relu'), 
  tf.keras.layers.Dense(256, activation='relu'), 
  tf.keras.layers.Dropout(0.3),
  tf.keras.layers.Dense(128, activation='relu'),
  tf.keras.layers.Dense(17, activation='softmax')
], name="DKS")

model.compile(
  optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),  # Lower learning rate
  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
  metrics=['accuracy'])
```

```{python}
history = model.fit(
  train_dataset,
  validation_data=val_dataset,
  epochs=70, 
  batch_size = 32
)
```

```{python}
from google.colab import drive
import os
drive.mount('/content/drive')





checkpoint_path = "/content/drive/MyDrive/Facial Recognition backups/checkpoint_{epoch:02d}.h5"
checkpoint_dir = "/content/drive/MyDrive/Facial Recognition backups"

if not os.path.exists(checkpoint_dir):
    os.makedirs(checkpoint_dir)

#checkpoint_filepath = '/tmp/checkpoint5.weights.h5'
model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
    filepath=checkpoint_path,
    save_weights_only=True,
    monitor='val_accuracy',
    mode='max',
    save_best_only=True)
```

